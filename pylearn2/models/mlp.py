"""
Extension Classes of MLP
"""
__authors__ = "Junyoung Chung"
__copyright__ = "Copyright 2014, Universite de Montreal"
__credits__ = "Junyoung Chung"
__license__ = "3-clause BSD"
__maintainer__ = "Junyoung Chung"
__email__ = "chungjun@iro"

import logging
import warnings

from librnn.pylearn2.expr.nnet import kl
from functools import wraps
from pylearn2.models.mlp import Layer, Sigmoid


logger = logging.getLogger(__name__)


class Sigmoid(Sigmoid):

    def __init__(self, monitor_style='detection', **kwargs):
        self._std_dev = kwargs.pop('noise_std_dev', .075)
        self.weight_noise = kwargs.pop('weight_noise', 0)
        super(Sigmoid, self).__init__(**kwargs)
        if monitor_style == 'classification':
            monitor_style = 'bit_vector_class'
            warnings.warn("The 'classification' monitor style is deprecated."
                          " Switch to 'bit_vector_class' (or possibly"
                          " 'one_hot_class' if your code predates 8f4b62b3df)."
                          " 'classification' may be removed on or after "
                          "2015-04-21.")
        assert monitor_style in ['one_hot_class', 'bit_vector_class',
                                 'detection']
        self.monitor_style = monitor_style

    @wraps(Layer.cost)
    def cost(self, Y, Y_hat):
        """
        Returns a batch (vector) of
        mean across units of KL divergence for each example.

        Parameters
        ----------
        Y : theano.gof.Variable
            Targets
        Y_hat : theano.gof.Variable
            Output of `fprop`

        mean across units, mean across batch of KL divergence
        Notes
        -----
        Uses KL(P || Q) where P is defined by Y and Q is defined by Y_hat
        Currently Y must be purely binary. If it's not, you'll still
        get the right gradient, but the value in the monitoring channel
        will be wrong.
        Y_hat must be generated by fprop, i.e., it must be a symbolic
        sigmoid.

        p log p - p log q + (1-p) log (1-p) - (1-p) log (1-q)
        For binary p, some terms drop out:
        - p log q - (1-p) log (1-q)
        - p log sigmoid(z) - (1-p) log sigmoid(-z)
        p softplus(-z) + (1-p) softplus(z)
        """
        total = self.kl(Y=Y, Y_hat=Y_hat)

        ave = total.sum()

        return ave

    def kl(self, Y, Y_hat):
        """
        Computes the KL divergence.


        Parameters
        ----------
        Y : Variable
            targets for the sigmoid outputs. Currently Y must be purely binary.
            If it's not, you'll still get the right gradient, but the
            value in the monitoring channel will be wrong.
        Y_hat : Variable
            predictions made by the sigmoid layer. Y_hat must be generated by
            fprop, i.e., it must be a symbolic sigmoid.

        Returns
        -------
        ave : Variable
            average kl divergence between Y and Y_hat.

        Notes
        -----
        Warning: This function expects a sigmoid nonlinearity in the
        output layer and it uses kl function under pylearn2/expr/nnet/.
        Returns a batch (vector) of mean across units of KL
        divergence for each example,
        KL(P || Q) where P is defined by Y and Q is defined by Y_hat:

        p log p - p log q + (1-p) log (1-p) - (1-p) log (1-q)
        For binary p, some terms drop out:
        - p log q - (1-p) log (1-q)
        - p log sigmoid(z) - (1-p) log sigmoid(-z)
        p softplus(-z) + (1-p) softplus(z)
        """
        batch_axis = self.output_space.get_batch_axis()
        div = kl(Y=Y, Y_hat=Y_hat, batch_axis=batch_axis)
        return div
